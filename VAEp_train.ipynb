{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE on precipitation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook demonstrates the process of training a Variational Autoencoder (VAE) on precipitation and sea-surface temperature data. The training process is divided into two steps: pre-training on CMIP6 data and transfer learning on observational data. The key steps and components involved are outlined as follows:\n",
    "\n",
    "1. The VAE model consists of four components: encoder, latent sampling, decoder, and a second decoder for prediction. Separate model instances are created for each component:\n",
    "     * _Encoder_: The encoder takes a sample `x` and returns the mean `z_mean` and logarithmic variance `z_log_var` of the latent variable `z`.\n",
    "    * _Latent Sampling_: The latent sampling takes `z_mean` and `z_log_var` as inputs and generates a random latent sample `z`.\n",
    "    * _Decoder_: The decoder reconstructs the input `x` by taking the latent sample `z` and producing the decoded output `y`. The decoding is done backward in time, maintaining the input order.\n",
    "   * _Decoder for Prediction_: The second decoder also takes the latent sample `z` but generates a forward-time prediction output.\n",
    "\n",
    "2. The full model is created by combining the four components. Model weights and training metrics are saved in a log directory (`LOG_DIR`). If the initial epoch (`INITIAL_EPOCH`) is greater than zero, the training continues from the saved weights; otherwise, a new training session starts.\n",
    "\n",
    "3. The data used in training includes CMIP data and observational data. CMIP data is loaded from netCDF files and grouped by model name and run. Observational data is also loaded from netCDF files. The datasets are split into training and validation sets.\n",
    "\n",
    "4. During pre-training, generators are prepared for training and validation on CMIP data and validation on observational data. A beta scheduler is used to scale the KL loss during training. Callbacks are set up to save model weights and metrics. The pre-training process is then initiated.\n",
    "\n",
    "5. For transfer learning, only observational data is used. Generators are prepared for training and validation. A modified model is built by freezing certain layers and keeping others trainable. The transfer learning process is initiated, and the model is trained on the observational data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The notebook requires the VAE package to be installed, which is available at:\n",
    "\n",
    "    https://github.com/andr-groth/VAE-project\n",
    "\n",
    "2. Sample data used in the notebook is included in the `data/` folder. The data is in netCDF format and has been prepared with the help of the CDO scripts, which are available at:\n",
    "\n",
    "    https://andr-groth.github.io/cdo-scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as ks\n",
    "import tensorflow.keras.backend as K\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from pytz import timezone\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import get_logger\n",
    "from tensorflow.compat.v1 import disable_eager_execution, disable_v2_behavior\n",
    "\n",
    "get_logger().setLevel('ERROR')\n",
    "disable_eager_execution()\n",
    "disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from VAE import generators, models\n",
    "from VAE.callbacks import Evaluate, ModelCheckpoint\n",
    "from VAE.utils import beta_schedulers, collection, fileio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['retina']\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['figure.dpi'] = 75\n",
    "np.set_printoptions(formatter={'float_kind': lambda x: f'{x: .3f}'}, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folder `LOG_DIR`, the model weights and training metrics are saved. The metrics can be monitored with Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INITIAL_EPOCH = 0\n",
    "LOG_DIR = r'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `INITIAL_EPOCH` to a value greater than zero will continue the training in `LOG_DIR`. Otherwise, a new training is started in a new subfolder created in `LOG_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL_EPOCH = 100\n",
    "# LOG_DIR = os.path.join(LOG_DIR, '2023-06-16T15.59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INITIAL_EPOCH == 0:\n",
    "    tz = timezone('Europe/Berlin')\n",
    "    log_dir = os.path.join(LOG_DIR, datetime.now(tz).strftime('%Y-%m-%dT%H.%M'))\n",
    "    os.makedirs(log_dir)\n",
    "    print('Start new training in:', os.path.normpath(log_dir))\n",
    "else:\n",
    "    log_dir = LOG_DIR\n",
    "    assert os.path.exists(log_dir), 'Path not found'\n",
    "    print(f'Continue training from epoch {INITIAL_EPOCH} in:', os.path.normpath(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the parameters of the model and the training. A first set of parameters (`model`, `data`, ...) is used for the pre-training and updated with a second set of parameters (`model2`, `data2`, ...) during transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INITIAL_EPOCH == 0:\n",
    "    pre_epochs = 10  # epochs in pre-training\n",
    "    tf_epochs = 100  # epochs in transfer learning\n",
    "    levels = [20, 20]  # number of PCs that will be used\n",
    "    input_shape = [12, sum(levels)]\n",
    "    prediction_shape = [12, sum(levels)]\n",
    "    ensemble_size = 40  # number of CMIP models\n",
    "    repeat_samples = 5  # ensemble members per batch\n",
    "    dtype = 'float32'\n",
    "\n",
    "    params = {\n",
    "        # params for encoder and decoder\n",
    "        'model': {\n",
    "            'activation': 'swish',\n",
    "            'beta': 'beta',\n",
    "            'cond_size': [12, ensemble_size],\n",
    "            'cond_units': 12,\n",
    "            'cond_ens_size': 6,\n",
    "            'cond_use_scale': False,\n",
    "            'delta': 1,\n",
    "            'encoder_blocks': 2,\n",
    "            'fc_units': 96,\n",
    "            'fc_activation': 'tanh',\n",
    "            'film_temporal': True,\n",
    "            'filters': 64,\n",
    "            'gamma': 10,\n",
    "            'input_shape': input_shape,\n",
    "            'latent_dim': 24,\n",
    "            'learning_rate': 1e-3,\n",
    "            'loss_weights': {\n",
    "                'decoder': 1,\n",
    "                'prediction': 5\n",
    "            },\n",
    "            'pooling': None,\n",
    "            'prediction_shape': prediction_shape,\n",
    "            'padding_blocks': 1,\n",
    "            'repeat_samples': repeat_samples,\n",
    "            'residual_units': 1,\n",
    "            'set_size': 1,\n",
    "            '__version__': models.__version__,\n",
    "        },\n",
    "        # specific params for prediction model\n",
    "        'prediction': {},\n",
    "        # specific params for transfer-learned model\n",
    "        'model2': {\n",
    "            'gamma': -1,\n",
    "            'learning_rate': 2e-4,\n",
    "            'loss_weights': {\n",
    "                'decoder': 1,\n",
    "                'prediction': 1\n",
    "            },\n",
    "            # 'trainable': ['*cond*', '*output_bn*'],\n",
    "            'trainable': ['*cond*', '*bn*'],\n",
    "        },\n",
    "        'data': {\n",
    "            'filename': [\n",
    "                'data/cmip6/historical/pr/pcs/pcs*.nc',\n",
    "                'data/cmip6/historical/tos/pcs/pcs*.nc',\n",
    "            ],\n",
    "            'level_range': [[level] for level in levels],\n",
    "            'time_interval': ['1850-01-01', '20014-12-31'],\n",
    "            'dtype': dtype\n",
    "        },\n",
    "        'data2': {\n",
    "            'filename': [\n",
    "                'data/gpcc/prj/pcs_anom_gpcc_v2020_1dgr.nc',\n",
    "                'data/ersst/prj/pcs_anom_ersstv5.nc',\n",
    "            ],\n",
    "            'level_range': [[level] for level in levels],\n",
    "            'time_interval': ['1891-01-01', '2019-12-31'],\n",
    "            'dtype': dtype\n",
    "        },\n",
    "        'beta_scheduler': {\n",
    "            'class_name': 'LogisticGrowth',\n",
    "            'config': {\n",
    "                'upper': 5,\n",
    "                'midpoint': 5,\n",
    "                'rate': 1,\n",
    "                'dtype': dtype\n",
    "            }\n",
    "        },\n",
    "        'beta_scheduler2': {},\n",
    "        'fit_generator': {\n",
    "            'input_length': input_shape[0],\n",
    "            'prediction_length': prediction_shape[0],\n",
    "            'batch_size': 128,\n",
    "            'ensemble_size': ensemble_size,\n",
    "            'ensemble_type': 'index',\n",
    "            'repeat_samples': repeat_samples,\n",
    "            'shuffle': True,\n",
    "            'strides': 1,\n",
    "            'tp_period': 12,\n",
    "            'validation_split': 40 + 32,\n",
    "            'dtype': dtype,\n",
    "            '__version__': generators.__version__,\n",
    "        },\n",
    "        'fit_generator2': {\n",
    "            'batch_size': 32,\n",
    "            'ensemble_sync': True,\n",
    "            'ensemble_type': 'random',\n",
    "            'initial_epoch': pre_epochs,\n",
    "            'validation_split': '1980-09-01',\n",
    "        },\n",
    "        'model_fit': {\n",
    "            'epochs': pre_epochs\n",
    "        },\n",
    "        'model_fit2': {\n",
    "            'epochs': tf_epochs\n",
    "        },\n",
    "    }\n",
    "\n",
    "    fn = os.path.join(log_dir, 'trainer_config.yaml')\n",
    "    with open(fn, 'w') as f:\n",
    "        yaml.dump(params, f)\n",
    "\n",
    "    print('Write configuration to:', os.path.normpath(fn))\n",
    "\n",
    "else:\n",
    "    fn = os.path.join(log_dir, 'trainer_config.yaml')\n",
    "    with open(fn, 'r') as f:\n",
    "        params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    print('Load configuration from:', os.path.normpath(fn))\n",
    "\n",
    "    assert params['model'].get('__version__') == models.__version__, 'Model version mismatch'\n",
    "    assert params['fit_generator'].get('__version__') == generators.__version__, 'Generator version mismatch'\n",
    "\n",
    "    params['fit_generator']['initial_epoch'] = INITIAL_EPOCH\n",
    "\n",
    "params = SimpleNamespace(**params)\n",
    "# print('')\n",
    "# print(yaml.dump(params.__dict__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE model consists of four components: encoder, latent sampling, decoder, and a second decoder for prediction. Separate model instances are created for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes a sample `x` and returns `z_mean` and `z_log_var`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.Encoder(**params.model, name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent sampling takes the two inputs `z_mean` and `z_log_var` and returns a set of `set_size=1` random latent sample `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_sampling = models.LatentSampling(**params.model, name='latent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder, finally, takes a latent sample `z` and returns the decoded output `y` to reconstruct `x`. The decoding works backward in time and we set `output_reverse=True` so that the order of decoder output matches the input to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = models.Decoder(output_shape=params.model.get('input_shape'),\n",
    "                         decoder_blocks=params.model.get('encoder_blocks'),\n",
    "                         output_reverse=True,\n",
    "                         **params.model,\n",
    "                         name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the decoder, the second decoder takes the same latent sample `z` and it's output will provide the prediction. In contrast to the `decoder`, we set `output_reverse=False` so that the output of `prediction` is forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = models.Decoder(output_shape=params.model.get('prediction_shape'),\n",
    "                            output_reverse=False,\n",
    "                            **{\n",
    "                                'decoder_blocks': params.model.get('encoder_blocks'),\n",
    "                                **params.model,\n",
    "                                **params.prediction\n",
    "                            },\n",
    "                            name='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the four components, we a ready to create the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.VAEp(encoder, decoder, latent_sampling, prediction, **params.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.utils.plot_model(model, show_shapes=True, dpi=75, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and summarizes the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### Load model weights (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we load model weights if `INITIAL_EPOCH > 0` to continue the pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if INITIAL_EPOCH > 0:\n",
    "    epoch = min(INITIAL_EPOCH, params.model_fit['epochs'])\n",
    "    model_file = os.path.join(log_dir, 'model.{epoch:02d}.h5'.format(epoch=epoch))\n",
    "    model.load_weights(model_file, by_name=True)\n",
    "    print('Load model weights from:', os.path.normpath(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we load the data for pre-training and transfer learning. The data is prepared with the CDO scripts. They are available on Github: https://andr-groth.github.io/cdo-scripts.\n",
    "\n",
    "For the pre-training, we use CMIP6 data. The raw gridded data is prepared with the `prepare_data.sh` script. The script performs three main steps: creating anomalies, calculating ensemble EOFs, and obtaining individual PCs. The resulting data is stored in netCDF files and the folder is specified in `params.data['filename']`.\n",
    "\n",
    "For the transfer learning, we use observational data. The raw gridded data is prepared with the `prepare_data2.sh` script. The script performs two main steps: creating anomalies and obtaining PCs. In contrast to the script `prepare_data.sh`, the EOFs are not calculated from the anomalies of the observational data. The anomalies are instead projected onto the ensemble EOFs calculated from the CMIP6 data to obtain the PCs. The resulting data is stored in netCDF files and the folder is specified in `params2.data['filename']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMIP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the netCDF files of CMIP data for pre-training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_variables, _dimensions, _attributes = fileio.read_netcdf_multi(**params.data, num2date=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the netCDF files and their variables by the global attributes  `source_id` + `variant_label`. The attribute `source_id` refers to the model name (e.g. `ACCESS-CM2`) and he attribute `variant_label` to the model run (e.g. `r1i1p1f1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {}\n",
    "key1 = 'source_id'\n",
    "key2 = 'variant_label'\n",
    "for data_name, values in _variables.items():\n",
    "    target_key = (\n",
    "        _attributes[data_name]['.'][key1],\n",
    "        _attributes[data_name]['.'][key2],\n",
    "    )\n",
    "\n",
    "    variables.setdefault(target_key, {})\n",
    "    variables[target_key] |= {k: pd.DataFrame(v, index=_dimensions[data_name]['time']) for k, v in values.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a few tests to check the integrity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = {tuple(val.keys()) for val in variables.values()}\n",
    "if len(variable_names) > 1:\n",
    "    raise ValueError(f'More than one variable combination found: {variable_names}')\n",
    "else:\n",
    "    variable_names, *_ = variable_names\n",
    "    print('\\N{check mark} One variable combination found:', variable_names)\n",
    "\n",
    "variable_channels = {tuple(v.shape[-1] for v in val.values()) for val in variables.values()}\n",
    "if len(variable_channels) > 1:\n",
    "    raise ValueError(f'More than one channel combination found: {variable_channels}')\n",
    "else:\n",
    "    variable_channels, *_ = variable_channels\n",
    "    print('\\N{check mark} One channel combination found:', variable_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes the models and their different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of model runs found :', len(variables))\n",
    "df = pd.crosstab(*list(zip(*list(variables.keys()))), rownames=[key1], colnames=[key2])\n",
    "df.loc['--- Total ---'] = df.sum(axis=0)\n",
    "display(df.replace(0, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables are stack along the last axis, the channels. We add a leading singleton dimension for `set_size=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_names = sorted(variables.keys(), key=lambda x: x[1])\n",
    "data_stack = [\n",
    "    pd.concat([variables[dataset_name][variable_name] for variable_name in variable_names], axis=1, join='inner')\n",
    "    for dataset_name in dataset_names\n",
    "]\n",
    "time = [d.index for d in data_stack]\n",
    "dataset = [d.to_numpy()[None, ...] for d in data_stack]\n",
    "\n",
    "print('Shapes of model runs :', {val.shape for val in dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the datasets into one set for training and one set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = params.fit_generator['validation_split']\n",
    "print('Size of training dataset   :', validation_split)\n",
    "print('Size of validation dataset :', len(dataset) - validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observational data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the netCDF data representing the observational data for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_variables2, _dimensions2, _attributes2 = fileio.read_netcdf_multi(**params.data2, num2date=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume a single set of different variables for observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables2 = {\n",
    "    k: pd.DataFrame(v, index=_dimensions2[name]['time'])\n",
    "    for name, values in _variables2.items() for k, v in values.items()\n",
    "}\n",
    "\n",
    "variable2_names = tuple(variables2.keys())\n",
    "variable2_channels = tuple([v.shape[-1] for v in variables2.values()])\n",
    "dataset2_names = 'Observations'\n",
    "\n",
    "print('Variables found :', variable2_names)\n",
    "print('Channel found   :', variable2_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stack the different variables along the last axis, the channel axis, and add a leading singleton dimension for `set_size=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_stack = pd.concat([variables2[variable2_name] for variable2_name in variable2_names], axis=1, join='inner')\n",
    "time2 = data2_stack.index\n",
    "dataset2 = data2_stack.to_numpy()[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the observations into two time intervals, one for training and one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split2 = np.searchsorted(time2, np.datetime64(params.fit_generator2['validation_split']))\n",
    "print(f\"Training interval   : {time2[:validation_split2][[0, -1]]}\")\n",
    "print(f\"Validation interval : {time2[validation_split2:][[0, -1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, we compare the CMIP datasets with the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 3\n",
    "rows = 3\n",
    "\n",
    "for variable_name, variable2_name in zip(variable_names, variable2_names):\n",
    "    fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(6 * cols, 3 * rows), squeeze=False)\n",
    "    fig.suptitle(variable_name.upper() + ' / ' + variable2_name.upper(), fontweight='bold')\n",
    "\n",
    "    for channel, (ax, value) in enumerate(zip(axs.flat, variables2[variable2_name].values.T)):\n",
    "        p2, = ax.plot(time2, value, color='tab:orange', zorder=2.2)\n",
    "        ax.grid(linestyle=':')\n",
    "        ax.set_title(f'Channel {channel}')\n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "        for channel, (ax, value) in enumerate(zip(axs.flat, variables[dataset_name][variable_name].values.T)):\n",
    "            p1, = ax.plot(variables[dataset_name][variable_name].index, value, color='tab:blue', alpha=0.5, zorder=1.1)\n",
    "\n",
    "    axs[0, -1].legend((p2, p1), ('Observation', 'CMIP'), loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the variance of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, len(variable_names), figsize=(16, 8), sharex=True, sharey='row', squeeze=False)\n",
    "for (uax, bax), variable_name, variable2_name in zip(axs.T, variable_names, variable2_names):\n",
    "    uax.set_title(variable_name.upper() + ' / ' + variable2_name.upper(), fontweight='bold')\n",
    "\n",
    "    data_var = np.stack([values[variable_name].var(axis=0) for values in variables.values()], axis=1)\n",
    "    p1, *_ = uax.plot(data_var, 'o', color='tab:blue', alpha=0.5, zorder=2.1)\n",
    "\n",
    "    data2_var = variables2[variable2_name].var(axis=0)\n",
    "    p2, = uax.plot(data2_var, 'o', color='tab:orange', markersize=10, zorder=2.2)\n",
    "\n",
    "    data_cvar = np.cumsum(data_var, axis=0)\n",
    "    bax.plot(data_cvar, '-o', color='tab:blue', alpha=0.5, zorder=2.1)\n",
    "\n",
    "    data2_cvar = np.cumsum(data2_var, axis=0)\n",
    "    bax.plot(data2_cvar, '-o', color='tab:orange', zorder=2.2)\n",
    "\n",
    "    uax.legend((p1, p2), ('CMIP', 'Observation'))\n",
    "    uax.set_ylabel('Variance')\n",
    "    uax.grid(linestyle=':')\n",
    "\n",
    "    bax.set_xlabel('Channel')\n",
    "    bax.set_ylabel('Cumulative variance')\n",
    "    bax.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model training, we first need a scheduler for the beta values in each epoch that scales the KL loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BetaScheduler = getattr(beta_schedulers, params.beta_scheduler.get('class_name'))\n",
    "beta_scheduler = BetaScheduler(**params.beta_scheduler.get('config'))\n",
    "beta_scheduler.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit generator on CMIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first generator takes the first set of the CMIP data. This generator is used to pre-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = [t.to_numpy().astype('datetime64[M]').astype(int) for t in time]\n",
    "_, model_index = np.unique([dataset_name[0] for dataset_name in dataset_names], return_inverse=True)\n",
    "fit_gen = generators.FitGenerator(dataset[:validation_split],\n",
    "                                  **params.fit_generator,\n",
    "                                  beta_scheduler=beta_scheduler,\n",
    "                                  time=month[:validation_split],\n",
    "                                  ensemble_index=model_index[:validation_split])\n",
    "fit_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation generator on CMIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second generator takes the second set of CMIP data. The generator is used to evaluate the sucess of the pre-training to generalize to independent CMIP runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset[validation_split:]:\n",
    "    val_gen = generators.FitGenerator(dataset[validation_split:],\n",
    "                                      **params.fit_generator,\n",
    "                                      beta_scheduler=beta_scheduler,\n",
    "                                      time=month[validation_split:],\n",
    "                                      ensemble_index=model_index[validation_split:])\n",
    "    val_gen.summary()\n",
    "else:\n",
    "    val_gen = None\n",
    "    print('No validation on CMIP data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validation generator on observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third generator takes the observational data. The generator is used to evaluate the success of the pre-training to model observational data. The validation scores are a proxy of a __zero-shot__ score on the observational data without transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month2 = time2.to_numpy().astype('datetime64[M]').astype(int)\n",
    "val_gen_obs = generators.FitGenerator(dataset2,\n",
    "                                      beta_scheduler=beta_scheduler,\n",
    "                                      time=month2,\n",
    "                                      **{\n",
    "                                          **params.fit_generator,\n",
    "                                          **params.fit_generator2\n",
    "                                      })\n",
    "val_gen_obs.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callbacks are used to save the model weights and evaluation metrics in the `LOG_DIR`. The progress of the pre-training can be monitored and analyzed with Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=os.path.join(log_dir, 'model.{epoch:02d}.h5'),\n",
    "                    period=5,\n",
    "                    save_best_only=False,\n",
    "                    save_weights_only=True),\n",
    "    Evaluate(val_gen_obs, prefix='val2_'),\n",
    "    ks.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=False),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start the pre-training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist = model.fit(fit_gen,\n",
    "                 validation_data=val_gen,\n",
    "                 initial_epoch=INITIAL_EPOCH,\n",
    "                 **params.model_fit,\n",
    "                 verbose=0,\n",
    "                 callbacks=callbacks + [TqdmCallback(verbose=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transfer learning, we only use the observational data.  Data prior to the validation split is used for the training and data after the validation split for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new beta schedular is created with an updated set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_p = {**params.beta_scheduler, **params.beta_scheduler2}\n",
    "BetaScheduler2 = getattr(beta_schedulers, bs_p['class_name'])\n",
    "beta_scheduler2 = BetaScheduler2(**bs_p['config'])\n",
    "beta_scheduler2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fit generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transfer learning, a first generator takes observational data prior to the validation split. This generator is used to continue the training of the modified model (see [Modify model](#Modify-model))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month2 = time2.to_numpy().astype('datetime64[M]').astype(int)\n",
    "fit_gen2 = generators.FitGenerator(dataset2[:, :validation_split2, :],\n",
    "                                   beta_scheduler=beta_scheduler2,\n",
    "                                   time=month2[:validation_split2],\n",
    "                                   **{\n",
    "                                       **params.fit_generator,\n",
    "                                       **params.fit_generator2\n",
    "                                   })\n",
    "fit_gen2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second generator takes observational data after the validation split and is used to evaluate the transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen2 = generators.FitGenerator(dataset2[:, validation_split2:, :],\n",
    "                                   beta_scheduler=beta_scheduler2,\n",
    "                                   time=month2[validation_split2:],\n",
    "                                   **{\n",
    "                                       **params.fit_generator,\n",
    "                                       **params.fit_generator2\n",
    "                                   })\n",
    "val_gen2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transfer learning, we only fine-tune a subset of layers in the model, while keeping all other layers frozen. The list of trainable layers can be modified with the `trainable` item in `params.model2` that is used to build the modifed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we build a new full model `model2` with the same `encoder`, `decoder`, and `prediction` from the pre-trained `model`, but with some layers set to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.VAEp(encoder, decoder, latent_sampling, prediction, **{**params.model, **params.model2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the summary below, we see that parameters are _trainable_ and _non-trainable_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes the trainable layers and their number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.summary_trainable(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### Load transfer-learned model weights (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model weights if `INITIAL_EPOCH` is greater than the pre-training epochs to continue the transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if (INITIAL_EPOCH > params.model_fit['epochs']):\n",
    "    model_file = os.path.join(log_dir, 'model.{epoch:02d}.h5'.format(epoch=INITIAL_EPOCH))\n",
    "    model.load_weights(model_file, by_name=True)\n",
    "    print('Load model weights from:', os.path.normpath(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start the transfer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist = model2.fit(fit_gen2,\n",
    "                  validation_data=val_gen2,\n",
    "                  initial_epoch=max(INITIAL_EPOCH, params.model_fit['epochs']),\n",
    "                  epochs=params.model_fit2['epochs'],\n",
    "                  callbacks=callbacks + [TqdmCallback(verbose=0)],\n",
    "                  verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
