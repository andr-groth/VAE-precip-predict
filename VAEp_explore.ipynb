{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore VAE on precipitation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook demonstrates the process of exploring a Variational Autoencoder (VAE) trained on precipitation data and exporting its predictions as netCDF files. The key steps and components involved are outlined as follows:\n",
    "\n",
    "1. The configuration parameters of the model are loaded from the `LOG_DIR` folder.\n",
    "\n",
    "2. The VAE model consists of four components: _encoder_, _latent sampling_, _decoder_, and a _second decoder for prediction_. Separate model instances are created for each component:\n",
    "    * _Encoder_: The encoder takes a sample `x` and returns the mean `z_mean` and logarithmic variance `z_log_var` of the latent variable `z`.\n",
    "    * _Latent Sampling_: The latent sampling takes `z_mean` and `z_log_var` as inputs and generates a random latent sample `z`.\n",
    "    * _Decoder_: The decoder reconstructs the input `x` by taking the latent sample `z` and producing the decoded output `y`. The decoding is done backward in time, maintaining the input order.\n",
    "   * _Decoder for Prediction_: The second decoder also takes the latent sample `z` but generates a forward-time prediction output.\n",
    "   \n",
    "3. The model weights from the training process are loaded from the `LOG_DIR` folder.\n",
    "   \n",
    "4. Observational data in netCDF format is loaded, with different variables stacked along the channel axis. The data is split into training and validation time intervals.\n",
    "\n",
    "5. Properties of the `encoder` and `decoder` are analyzed. KL divergence of latent variables is analyzed to identify important dimensions. The temporal behavior of latent variables is also examined using the validation dataset.\n",
    "\n",
    "6. The model outputs are obtained for the validation data. The outputs of the `decoder` and `prediction` are collected separately and aligned with the target month. The VAE output is restricted to specific time lags for reducing file size.\n",
    "\n",
    "7. The model output is projected into grid space by calculating the scalar product of the output with EOFs (Empirical Orthogonal Functions). The corresponding climatological mean fields are loaded and added to obtain absolute values. The resulting grid-space reconstruction is exported as netCDF files in the specified `EXPORT_DIR` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as ks\n",
    "import yaml\n",
    "from matplotlib import dates\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import get_logger\n",
    "from tensorflow.compat.v1 import disable_eager_execution, disable_v2_behavior\n",
    "\n",
    "get_logger().setLevel('ERROR')\n",
    "disable_eager_execution()\n",
    "disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from VAE import generators, models\n",
    "from VAE.utils import fileio\n",
    "from VAE.utils import plot as vplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FIGWIDTH = 16\n",
    "VERBOSE = 1\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['figure.dpi'] = 75\n",
    "np.set_printoptions(formatter={'float_kind': lambda x: f'{x: .3f}'}, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the configuration from the the folder `LOG_DIR`. The model output is written to netCDF files in the folder given in `EXPORT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "LOG_DIR = r'logs/2023-06-16T15.59'\n",
    "MODEL_FILE = f'model.{EPOCH:02d}.h5'\n",
    "EXPORT_DIR = r'results/2023-06-16T15.59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('LOG_DIR    :', LOG_DIR)\n",
    "print('MODEL_FILE :', MODEL_FILE)\n",
    "print('EXPORT_DIR :', EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the parameters from the model training in `trainer_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(LOG_DIR, 'trainer_config.yaml')\n",
    "with open(fn, 'r') as f:\n",
    "    params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "print('Load configuration from:', os.path.normpath(fn))\n",
    "\n",
    "assert params['model'].get('__version__') == models.__version__, 'Model version mismatch.'\n",
    "assert params['fit_generator'].get('__version__') == generators.__version__, 'Generator version mismatch.'\n",
    "\n",
    "params = SimpleNamespace(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some modifications to the parameters. To change the number of model runs that are used to obtain the ensemble statistics exported as netCDF, we can change the parameter `repeat_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.model['beta'] = 1.  # no beta scheduler needed at inference time\n",
    "params.fit_generator2['shuffle'] = False  # do not shuffle samples\n",
    "params.fit_generator2['ensemble_replace'] = True  # sample from ensemble index with replacement\n",
    "\n",
    "params.fit_generator2['batch_size'] = 10  # reduce batch size if increasing number of repetitions\n",
    "params.fit_generator2['repeat_samples'] = 32  # set number of repetitions = model runs\n",
    "\n",
    "if EPOCH <= params.model_fit['epochs']:\n",
    "    params.model2 = {}  # remove model params of TF learning if model weights of pre-trained model will be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(yaml.dump(params.__dict__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE model consists of four components: encoder, latent sampling, decoder, and a second decoder for prediction. Separate model instances are created for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes a sample `x` and returns `z_mean` and `z_log_var`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.Encoder(**params.model, name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent sampling takes the two inputs `z_mean` and `z_log_var` and returns a set of `set_size=1` random latent sample `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_sampling = models.LatentSampling(**params.model, name='latent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder, finally, takes a latent sample `z` and returns the decoded output `y` to reconstruct `x`. The decoding works backward in time and we set `output_reverse=True` so that the order of decoder output matches the input to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = models.Decoder(output_shape=params.model.get('input_shape'),\n",
    "                         decoder_blocks=params.model.get('encoder_blocks'),\n",
    "                         output_reverse=True,\n",
    "                         **params.model,\n",
    "                         name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the decoder, the second decoder takes the same latent sample `z` and it's output will provide the prediction. In contrast to the `decoder`, we set `output_reverse=False` so that the output of `prediction` is forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = models.Decoder(output_shape=params.model.get('prediction_shape'),\n",
    "                            output_reverse=False,\n",
    "                            **{\n",
    "                                'decoder_blocks': params.model.get('encoder_blocks'),\n",
    "                                **params.model,\n",
    "                                **params.prediction\n",
    "                            },\n",
    "                            name='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the four components, we a ready to create the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.VAEp(encoder, decoder, latent_sampling, prediction, **{**params.model, **params.model2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.utils.plot_model(model, show_shapes=True, dpi=75, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and summarizes the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### Load model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model weights from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fn = os.path.join(LOG_DIR, MODEL_FILE)\n",
    "model.load_weights(fn, by_name=True)\n",
    "print('Load model weights from:', os.path.normpath(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the netCDF data representing the observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_variables, _dimensions, _attributes = fileio.read_netcdf_multi(**params.data2, num2date=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume a single set of different variables for observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    k: pd.DataFrame(v, index=_dimensions[key]['time'])\n",
    "    for key, value in _variables.items() for k, v in value.items()\n",
    "}\n",
    "\n",
    "dimensions = {k: _dimensions[key] for key, value in _variables.items() for k in value}\n",
    "attributes = {k: _attributes[key] for key, value in _variables.items() for k in value}\n",
    "\n",
    "variable_names = tuple(variables.keys())\n",
    "variable_channels = tuple([v.shape[-1] for v in variables.values()])\n",
    "\n",
    "print('Variables found :', variable_names)\n",
    "print('Channel found   :', variable_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stack the different variables along the last axis, the channel axis, and add a leading singleton dimension for `set_size=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stack = pd.concat([variables[variable2_name] for variable2_name in variable_names], axis=1, join='inner')\n",
    "time = data_stack.index\n",
    "dataset = data_stack.to_numpy()[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the observations into two time intervals, one for training and one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = np.searchsorted(time, np.datetime64(params.fit_generator2['validation_split']))\n",
    "print(f\"Training interval   : {time[:validation_split][[0, -1]]}\")\n",
    "print(f\"Validation interval : {time[validation_split:][[0, -1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, we show the observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 3\n",
    "rows = 3\n",
    "\n",
    "for variable_name in variable_names:\n",
    "    fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(6 * cols, 3 * rows), squeeze=False)\n",
    "    fig.suptitle(variable_name.upper(), fontweight='bold')\n",
    "\n",
    "    for channel, (ax, value) in enumerate(zip(axs.flat, variables[variable_name].values.T), start=1):\n",
    "        p2, = ax.plot(time, value, color='tab:orange', zorder=2.2)\n",
    "        ax.grid(linestyle=':')\n",
    "        ax.set_title(f'Channel {channel}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator takes observational data after the validation split and is used to evaluate the transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = time.to_numpy().astype('datetime64[M]').astype(int)\n",
    "val_gen = generators.FitGenerator(dataset[:, validation_split:, :],\n",
    "                                  time=month[validation_split:],\n",
    "                                  **{\n",
    "                                      **params.fit_generator,\n",
    "                                      **params.fit_generator2\n",
    "                                  })\n",
    "val_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The module `VAE.utils.plot` provides multiple functions to plot and analyze properties of the `encoder` and the `decoder`. First let's start with the `encoder` and explore properties of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0, figsize=(FIGWIDTH, 6))\n",
    "fig, ax, z_order, kl_div = vplt.encoder_boxplot(encoder, val_gen, plottype='kl', name=0, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the KL divergence of the latent variables for each of the latent dimension separately. The dimensions are sorted in descending order of the KL divergence. Latent dimensions with a high KL divergence are more important for the reconstruction with the decoder. Latent dimensions that have a KL divergence close to zero are unused dimensions; i.e. they are practically not important for the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Temporal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we analyze the temporal behavior of the latent variables. In doing so, we obtain the latent variables of the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var = encoder.predict(val_gen, verbose=VERBOSE)\n",
    "z_sample = latent_sampling.predict([z_mean, z_log_var])\n",
    "z_sample = np.squeeze(z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = 7\n",
    "fig, axs = plt.subplots(rows,\n",
    "                        2,\n",
    "                        figsize=(FIGWIDTH, 2 * rows),\n",
    "                        sharex='col',\n",
    "                        sharey='col',\n",
    "                        squeeze=False,\n",
    "                        gridspec_kw={\n",
    "                            'width_ratios': [2.5, 1],\n",
    "                            'wspace': 0.1\n",
    "                        })\n",
    "\n",
    "r = val_gen.repeat_samples\n",
    "nfft = 2**12\n",
    "fs = 12\n",
    "\n",
    "t = time[validation_split:]\n",
    "\n",
    "for (lax, rax), k in zip(axs, z_order):\n",
    "    lax.plot(t[val_gen.input_length:-val_gen.prediction_length + 1],\n",
    "             z_sample[:, k].reshape(-1, r),\n",
    "             '.',\n",
    "             markersize=1,\n",
    "             color='tab:cyan')\n",
    "    lax.plot(t[val_gen.input_length:-val_gen.prediction_length + 1],\n",
    "             z_sample[:, k].reshape(-1, r).mean(axis=-1),\n",
    "             color='tab:orange')\n",
    "    lax.set_ylabel(f'{k=}')\n",
    "    lax.grid(axis='x', linestyle=':')\n",
    "\n",
    "    f, pxx = signal.welch(z_sample[:, k].reshape(-1, r), nfft=nfft, fs=fs, nperseg=512, axis=0, scaling='spectrum')\n",
    "    rax.plot(f, pxx, '.', markersize=1, color='tab:cyan')\n",
    "    rax.plot(f, pxx.mean(axis=-1), color='tab:orange')\n",
    "\n",
    "lax.margins(x=0.005)\n",
    "lax.xaxis.set_major_locator(dates.YearLocator(10))\n",
    "lax.xaxis.set_major_formatter(dates.DateFormatter('%Y'))\n",
    "lax.xaxis.set_minor_locator(dates.YearLocator(2))\n",
    "lax.set_xlabel('Time')\n",
    "\n",
    "rax.set_xlim((0, 0.7))\n",
    "rax.xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "rax.xaxis.set_minor_locator(ticker.AutoMinorLocator(4))\n",
    "_ = rax.set_xlabel('Cycles per year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first obtain the model outputs for the input data to `val_gen`. The output of `decoder` is returned in `xcs` and the output of `prediction` in `ycs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcs, ycs = model.predict(val_gen, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we replace the output values in `ycs` at lead zero with the actually input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = [targets['prediction'] for inputs, targets in val_gen]\n",
    "y_target = np.concatenate(y_target)\n",
    "ycs[..., 0, :] = y_target[..., 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `decoder` and `prediction` outputs are concatenated along the lead/lag dimension and the singleton dimension for `set_size=1` is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcs = np.concatenate([xcs, ycs], axis=2)\n",
    "xcs = np.squeeze(xcs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the model output is aligned with the target month and split into the different variables. To reduce the later size of the netCDF files, we restrict the VAE output to specific time lags given in `export_lags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_lags = np.arange(-1, val_gen.prediction_length)\n",
    "# export_lags = np.arange(-val_gen.input_length, val_gen.prediction_length)\n",
    "export_lags = [-1, 0, 2, 5, 8, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange runs (repeat samples) along second axis\n",
    "r = val_gen.repeat_samples\n",
    "xcs_runs = xcs.reshape(-1, r, *xcs.shape[1:])\n",
    "\n",
    "level = np.arange(-val_gen.input_length, val_gen.prediction_length)\n",
    "lag_idx = val_gen.input_length + np.array(export_lags)\n",
    "\n",
    "# select lags\n",
    "level = level[lag_idx]\n",
    "xcs_runs = xcs_runs[:, :, lag_idx, :]\n",
    "\n",
    "# align  with target month\n",
    "xcs_runs = np.pad(xcs_runs,\n",
    "                  pad_width=((val_gen.input_length, val_gen.prediction_length - 1), (0, 0), (0, 0), (0, 0)),\n",
    "                  mode='constant',\n",
    "                  constant_values=np.nan)\n",
    "\n",
    "xcs_runs = np.stack([np.roll(xcs_runs[:, :, n, :], lag, axis=0) for n, lag in enumerate(level)], axis=2)\n",
    "\n",
    "# split channels into variables\n",
    "channel_splits = np.cumsum(variable_channels)\n",
    "splits = np.split(xcs_runs, channel_splits, axis=-1)\n",
    "\n",
    "xcs_variables = dict(zip(variable_names, splits))\n",
    "xcs_dimensions = {variable_name: {'time': time[validation_split:], 'level': level} for variable_name in variable_names}\n",
    "xcs_attributes = {\n",
    "    variable_name: {\n",
    "        'level': {\n",
    "            'long_name': 'Time lag',\n",
    "            'units': '',\n",
    "            'axis': 'Z'\n",
    "        }\n",
    "    }\n",
    "    for variable_name in variable_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction in grid space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the model output is projected into the grid space by forming the scalar product of the model output with the EOFs. The result is exported as netCDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EOFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the EOFs from the `eofs.nc` files, which can also be found in the data folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eof_files = [os.path.join(os.path.dirname(filename), 'eofs.nc') for filename in params.data2['filename']]\n",
    "_eof_variables, _eof_dimensions, _eof_attributes = fileio.read_netcdf_multi(filename=eof_files,\n",
    "                                                                            time_range=params.data.get('level_range'),\n",
    "                                                                            dtype=params.data2.get('dtype'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eof_variables = {}\n",
    "eof_dimensions = {}\n",
    "eof_attributes = {}\n",
    "for key, value in _eof_variables.items():\n",
    "    eof_variables |= value\n",
    "\n",
    "    eof_dimensions |= {k: _eof_dimensions[key] for k in value}\n",
    "    eof_attributes |= {k: _eof_attributes[key] for k in value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EOF variables  :', list(eof_variables))\n",
    "print('Data variables :', list(xcs_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load climatological mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain absolute values, we also load the corresponding climatological mean fields. The netCDF files will be looked up in the `mean_path` folder, relative to the data folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_path = '../mean/*.nc'\n",
    "mean_files = [os.path.join(os.path.dirname(filename), mean_path) for filename in params.data2['filename']]\n",
    "_mean_variables, _mean_dimensions, _mean_attributes = fileio.read_netcdf_multi(filename=mean_files,\n",
    "                                                                               num2date=True,\n",
    "                                                                               dtype=params.data2.get('dtype'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_variables = {}\n",
    "mean_dimensions = {}\n",
    "mean_attributes = {}\n",
    "for key, value in _mean_variables.items():\n",
    "    mean_variables |= value\n",
    "\n",
    "    mean_dimensions |= {k: _mean_dimensions[key] for k in value}\n",
    "    mean_attributes |= {k: _mean_attributes[key] for k in value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to netCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We form the dot product of the model outputs with the EOFs and add the climatological mean. Optionally, the log transform is reverted. Different ensemble statistics are obtained from the stochastic output of the VAE. The result is written to netCDF files in the folder given in `EXPORT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(EXPORT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prcs = {'ensmedian': 50, 'enspctl10': 10, 'enspctl90': 90}\n",
    "\n",
    "for (data_key, value), (eof_key, eof) in zip(xcs_variables.items(), eof_variables.items()):\n",
    "    print('-' * 3, data_key, '-' * (77 - len(data_key)))\n",
    "    filename = '{prefix:s}' + data_key + '.{type:s}.nc'\n",
    "    filename = os.path.join(EXPORT_DIR, filename)\n",
    "    nc_dimensions = mean_dimensions[data_key] | dimensions[data_key] | xcs_dimensions[data_key]\n",
    "    nc_attributes = mean_attributes[data_key] | attributes[data_key] | xcs_attributes[data_key]\n",
    "    kwargs = dict(dimensions=nc_dimensions, attributes=nc_attributes)\n",
    "\n",
    "    # scalar product\n",
    "    nc_variable = np.tensordot(value, eof, axes=1)\n",
    "\n",
    "    # save anomalies\n",
    "    fileio.write_netcdf(filename.format(prefix='anom_', type='ensmean'),\n",
    "                        variables={data_key: np.mean(nc_variable, axis=1)},\n",
    "                        **kwargs)\n",
    "\n",
    "    nc_prcs = np.percentile(nc_variable, list(prcs.values()), axis=1)\n",
    "    for type, value in zip(prcs, nc_prcs):\n",
    "        fileio.write_netcdf(filename.format(prefix='anom_', type=type), variables={data_key: value}, **kwargs)\n",
    "\n",
    "    # revert to absolute values\n",
    "    months = pd.to_datetime(xcs_dimensions[data_key]['time']).month\n",
    "    mean_months = pd.to_datetime(mean_dimensions[data_key]['time']).month\n",
    "    for month in mean_months:\n",
    "        nc_variable[months == month, ...] += mean_variables[data_key][mean_months == month, ...]\n",
    "\n",
    "    # invert log transform\n",
    "    if '-log ' in eof_attributes[eof_key]['.']['history']:\n",
    "        nc_variable = np.exp(nc_variable)\n",
    "\n",
    "    # save absolute values\n",
    "    fileio.write_netcdf(filename.format(prefix='', type='ensmean'),\n",
    "                        variables={data_key: np.mean(nc_variable, axis=1)},\n",
    "                        **kwargs)\n",
    "\n",
    "    nc_prcs = np.percentile(nc_variable, list(prcs.values()), axis=1)\n",
    "    for type, value in zip(prcs, nc_prcs):\n",
    "        fileio.write_netcdf(filename.format(prefix='', type=type), variables={data_key: value}, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we derive predictions of cumulative rainfall indices covering the growing seasons of the main crops. We first load the data on the growing season from the `data/crop/` folder. The data is assumed to be on the same grid as the EOFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_file = 'data/crop/soy_rf_ggcmi_crop_calendar_phase3_v1.01_1dgr.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_variables, _, _ = fileio.read_netcdf(filename=crop_file)\n",
    "print('Available data:', {k: v.shape for k, v in crop_variables.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the netCDF files with the VAE output for further post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(EXPORT_DIR, variable_names[0] + '*.nc')\n",
    "result_variables, result_dimensions, result_attributes = fileio.read_netcdf_multi(filename=filename, num2date=True)\n",
    "print(*list(result_variables.keys()), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planting_day = crop_variables['planting_day']\n",
    "growing_season_length = crop_variables['growing_season_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the cumulative values for each of the loaded VAE outputs and write the results to netCDF files with the prefix `cum_` preprended to the filename. THe files are saved in the same folder as the VAE output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, values in result_variables.items():\n",
    "    # monthly values refer to end month (M = month end frequency)\n",
    "    source_time = pd.to_datetime(result_dimensions[filename]['time']).snap('M')\n",
    "    # new time is Jan-01 of each year (YS = year start frequency)\n",
    "    target_time = source_time.snap('YS').unique()\n",
    "\n",
    "    # iterate over variables\n",
    "    out_values = dict()\n",
    "    for key, value in values.items():\n",
    "        out_value = np.full((len(target_time), *value.shape[1:]), fill_value=np.nan, dtype=value.dtype)\n",
    "\n",
    "        # iterate over grid points\n",
    "        pbar = ks.utils.Progbar(value.shape[-1], unit_name='Longitude')\n",
    "        for lon_idx in range(value.shape[-1]):\n",
    "            pbar.add(1)\n",
    "            for lat_idx in range(value.shape[-2]):\n",
    "                if np.all(np.isnan(value[..., lat_idx, lon_idx])):\n",
    "                    continue\n",
    "\n",
    "                # converting data to Dataframe makes datetime manipulations easier\n",
    "                df = pd.DataFrame(value[..., lat_idx, lon_idx], index=source_time)\n",
    "\n",
    "                # get total cumulative rainfall at grid point\n",
    "                df = df.cumsum(axis=0)\n",
    "\n",
    "                # interpolate on daily time scales to account for fraction of month (slower computation!)\n",
    "                df = df.asfreq('D').interpolate('linear')\n",
    "\n",
    "                # get start and end dates of crop seasons\n",
    "                start_time = target_time + pd.to_timedelta(planting_day[lat_idx, lon_idx] - 1, unit='D')\n",
    "                end_time = start_time + pd.to_timedelta(growing_season_length[lat_idx, lon_idx], unit='D')\n",
    "\n",
    "                # get cumulative rainfall from difference between end and start of crop season\n",
    "                start_value = df.reindex(index=start_time, method='nearest').to_numpy()\n",
    "                end_value = df.reindex(index=end_time, method='nearest').to_numpy()\n",
    "                out_value[..., lat_idx, lon_idx] = end_value - start_value\n",
    "\n",
    "        out_values[key] = out_value\n",
    "\n",
    "    out_filename = os.path.join(os.path.dirname(filename), 'cum_' + os.path.basename(filename))\n",
    "    fileio.write_netcdf(out_filename,\n",
    "                        variables=out_values,\n",
    "                        dimensions=result_dimensions[filename] | {'time': target_time},\n",
    "                        attributes=result_attributes[filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark on country averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract country averages from the gridded cumulative rainfall data, we can use the CDO operators `fldmean` and `maskregion`. For example, we can use the following command\n",
    "\n",
    "```shell\n",
    "cdo fldmean -maskregion,dcw:TZ infile.nc outfile.nc\n",
    "```\n",
    "to extract the country average for Tanzania. To extract the country average for another country, we can replace `TZ` with the corresponding country code. For more details, see the [CDO documentation](https://code.mpimet.mpg.de/projects/cdo/embedded/cdo.pdf) and the list of [country codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2).\n",
    "\n",
    "To use country codes together with the `maskregion` operator, we need to install the `gmt-dcw` package, which is available in the Ubuntu repositories. To install the package, run `sudo apt install gmt-dcw` and set the environment variable `DCW_DIR` to the path of the `dcw-gmt` folder, e.g. `export DIR_DCW=/usr/share/gmt-dcw/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
